{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/UPM/EscPolitecnica/EscUpmPolit_p.gif \"UPM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo final SITC\n",
    "## Análisis de sentimientos en Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Departamento de Ingeniería de Sistemas Telemáticos. Universidad Politécnica de Madrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizado por:\n",
    "- Juan Bermudo Mera\n",
    "- Margarita Bolívar Jiménez\n",
    "- Lourdes Fernández Nieto\n",
    "- Ramón Pérez Hernández\n",
    "\n",
    "© 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo RandomForest aplicado sobre el fichero de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de contenidos\n",
    "\n",
    "* [Importación de datos necesarios para aplicar el algoritmo](#1.-Importación-de-datos-necesarios-para-aplicar-el-algoritmo)\n",
    "\t* [Importación de librerías](#Importación-de-librerías)\n",
    "    * [Importación de corpus y tweets](#Importación-de-corpus-y-tweets)\n",
    "    * [Tokenización y stemming](#Tokenización-y-stemming)\n",
    "* [Entrenamiento del modelo](#2.-Entrenamiento-del-modelo)\n",
    "* [Rendimiento del modelo](#3.-Rendimiento-del-modelo)\n",
    "* [Predicción de la polaridad](#4.-Predicción-de-la-polaridad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de datos necesarios para aplicar el algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lourdes/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Lourdes/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías. Las que no están instaladas, instalar con pip install <nombre_paquete>\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Importación de corpus y tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Se importan el corpus y los tweets\n",
    "tweets_df = pd.read_excel('ficheros/Preprocesados/tweets_corpus_header.xlsx', header=0, encoding='iso8859_15')\n",
    "tweets = pd.read_excel('ficheros/TweetsConTopic/tweets_spainGeo_topic.xlsx', header=0, encoding='iso8859_15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Tokenización y stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Lourdes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Se descargan las palabras de parada en español\n",
    "nltk.download(\"stopwords\")\n",
    "spanish_stopwords = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtenemos los signos de puntuación que se utilizan en español\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se definen las funciones para realizar la tokenización y el stemming\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # Eliminamos lo que no sean palabras\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    # Tokenización\n",
    "    tokens = tknzr.tokenize(text)\n",
    "\n",
    "    # Stemming\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_split': 1e-07,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': 1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buscamos los parámetros que podemos utilizar para entrenar el modelo\n",
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['de', 'la'...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'cls__criterion': ('gini', 'entropy'), 'cls__n_estimators': (1, 10, 100, 1000), 'cls__class_weight': ['balanced', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el pipeline para poder encadenar todos los elementos necesarios para realizar la estimación.\n",
    "# Realizamos la búsqueda mediante GridSearhCV, que es una librería de sklearn que permite realizar una\n",
    "# búsqueda de los mejores parámetros del modelo, utulizando los parámetros definidos en parameters y como\n",
    "# métrica, roc_auc (área bajo la curva ROC)\n",
    "pipeline = Pipeline([\n",
    "    ('vect',  CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)),\n",
    "    ('cls', RandomForestClassifier())])\n",
    "parameters = {\n",
    "    'cls__criterion': ('gini','entropy'),\n",
    "    'cls__n_estimators': (1, 10, 100, 1000),\n",
    "    'cls__class_weight':['balanced', None]    \n",
    "}\n",
    "gs = GridSearchCV(pipeline, parameters, n_jobs=-1, scoring='roc_auc')\n",
    "gs.fit(tweets_df.content, tweets_df.polarity_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls__class_weight': 'balanced',\n",
       " 'cls__criterion': 'entropy',\n",
       " 'cls__n_estimators': 1000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos los mejores parámetros del SVC obtenidos de la búsqueda con GridSearchCV \n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Los mejores parámetros que encuentra GridSearchCV para el SVC utilizando la métrica roc_auc son:\n",
    "class_weight = balanced,\n",
    "criterion = 'entropy',\n",
    "n_estimators = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ResultadosGridSearch/grid_searchRandomForest.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardamos el resultado del GridSearchCV en un fichero de manera persistente\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(gs, 'ficheros/ResultadosGridSearch/grid_searchRandomForest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## 3. Rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82515864301467035"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mediante validación cruzada obtenemos el rendimiento del modelo\n",
    "model = RandomForestClassifier(class_weight = 'balanced', criterion = 'entropy', n_estimators = 1000)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = spanish_stopwords,\n",
    "    min_df = 0,\n",
    "    max_df = 4700,\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "tweets_df_data_features = vectorizer.fit_transform(tweets_df.content)\n",
    "tweets_df_data_features_nd = tweets_df_data_features.toarray()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    model,\n",
    "    tweets_df_data_features_nd[0:len(tweets_df)],\n",
    "    y=tweets_df.polarity_bin,\n",
    "    scoring='roc_auc',\n",
    "    cv=None\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El valor que se obtiene del rendimiento del modelo para la métrica de Área bajo la Curva ROC es de 0.82515864301467035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Predicción de la polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Una vez que tenemos el modelo que mejor métrica nos aporta (tras realizar muchas pruebas con distintas métricas \n",
    "# y parámetros pasados al modelo), volvemos a crear un pipeline pero en este caso, pasándole los mejores parámetros\n",
    "# obtenidos del SVC para predecir qué polaridad tienen los tweets que están aún sin etiquetar\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords,\n",
    "            min_df = 0,\n",
    "            max_df = 26363,\n",
    "            max_features=1000\n",
    "            )),\n",
    "    ('cls', RandomForestClassifier(class_weight = 'balanced', criterion = 'entropy', n_estimators = 1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(tweets_df.content, tweets_df.polarity_bin)\n",
    "tweets['polarity'] = pipeline.predict(tweets.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18347</th>\n",
       "      <td>Este programa nos lo mereciamos todos los q he...</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>Nos encantó la #vela en #NiñaDePapa #madrid #r...</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>El Libro Biblia   #biblia #libro #el</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5113</th>\n",
       "      <td>@AlvaroLario El alma... ;)</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>@gabikrn123 hi. imagino que te gustara estas w...</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>El Mundo, El País, La SER, ABC, OKDiario... Pa...</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8485</th>\n",
       "      <td>@AndresMarchante No he visto nada igual en 20 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26348</th>\n",
       "      <td>13/04/16 #paseandoanaya #iphone5 #primavera #s...</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8528</th>\n",
       "      <td>Y si me va mal pues me piro pa un correccional</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12158</th>\n",
       "      <td>De la fuente family @ Navalcarnero Ciudad</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12583</th>\n",
       "      <td>Yupi yopi yepi yapi... Me aburro en el aeropue...</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>Si no entiendes el Modelo de #Negocio de una #...</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>Las pernoctaciones hoteleras crecen un 17,6% e...</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>Que no le vi en mi puta vida, pero era gilipol...</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>Si quieres encontrarnos ..\\nestamos en la luch...</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>@manuelgc541 vete a pellizcar cristales un rat...</td>\n",
       "      <td>0</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16705</th>\n",
       "      <td>Hoy ha tocado series en pista.\\n4x400 - 7x200 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>@AlvaroEllakuria @Ruth__Diaz @dasjoshua @pacot...</td>\n",
       "      <td>1</td>\n",
       "      <td>gracias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10311</th>\n",
       "      <td>#RETENCIÓN nivel AMARILLO en AUTOPISTA / AUTOV...</td>\n",
       "      <td>0</td>\n",
       "      <td>trafico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>@MuyLiberal @curritosuarez bueno, igual alguno...</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  polarity    Topic\n",
       "18347  Este programa nos lo mereciamos todos los q he...         0    otros\n",
       "10503  Nos encantó la #vela en #NiñaDePapa #madrid #r...         1    otros\n",
       "4501                El Libro Biblia   #biblia #libro #el         1    otros\n",
       "5113                          @AlvaroLario El alma... ;)         0    otros\n",
       "5899   @gabikrn123 hi. imagino que te gustara estas w...         1    otros\n",
       "6461   El Mundo, El País, La SER, ABC, OKDiario... Pa...         1    otros\n",
       "8485   @AndresMarchante No he visto nada igual en 20 ...         1    otros\n",
       "26348  13/04/16 #paseandoanaya #iphone5 #primavera #s...         0    otros\n",
       "8528      Y si me va mal pues me piro pa un correccional         0    otros\n",
       "12158         De la fuente family @ Navalcarnero Ciudad          1    otros\n",
       "12583  Yupi yopi yepi yapi... Me aburro en el aeropue...         0    otros\n",
       "961    Si no entiendes el Modelo de #Negocio de una #...         1    otros\n",
       "3639   Las pernoctaciones hoteleras crecen un 17,6% e...         0    otros\n",
       "5345   Que no le vi en mi puta vida, pero era gilipol...         0    otros\n",
       "16410  Si quieres encontrarnos ..\\nestamos en la luch...         0    otros\n",
       "8368   @manuelgc541 vete a pellizcar cristales un rat...         0    otros\n",
       "16705  Hoy ha tocado series en pista.\\n4x400 - 7x200 ...         1    otros\n",
       "7937   @AlvaroEllakuria @Ruth__Diaz @dasjoshua @pacot...         1  gracias\n",
       "10311  #RETENCIÓN nivel AMARILLO en AUTOPISTA / AUTOV...         0  trafico\n",
       "716    @MuyLiberal @curritosuarez bueno, igual alguno...         1    otros"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos algunos de los tweets que han sido etiquetados con la polaridad\n",
    "tweets[['content', 'polarity','Topic']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Guardamos los tweets en un fichero csv con su polaridad\n",
    "tweets[[ 'content', 'Latitude', 'Longitude', 'polarity','Topic']].to_csv('ficheros/TweetsConPolaridadYTopic/tweetsRandomForest_polarity_bin.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Guardamos los tweets en fichero excel\n",
    "tweets[['content','Latitude','Longitude','polarity','Topic']].to_excel('ficheros/TweetsConPolaridadYTopicExcel/tweetsRandomForest_polarity_bin.xlsx', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El notebook está licenciado libremente bajo la licencia [Creative Commons Attribution Share-Alike](https://creativecommons.org/licenses/by/2.0/).\n",
    "\n",
    "La base del código empleado procede del trabajo de Manuel Garrido llamado [Cómo hacer Análisis de Sentimiento en español](http://pybonacci.org/2015/11/24/como-hacer-analisis-de-sentimiento-en-espanol-2/).\n",
    "\n",
    "© 2017 - Juan Bermudo Mera, Margarita Bolívar Jiménez, Lourdes Fernández Nieto, Ramón Pérez Hernández.\n",
    "\n",
    "Universidad Politécnica de Madrid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
